{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <p align=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Logo-gustave-roussy.jpg/1200px-Logo-gustave-roussy.jpg\" alt=\"Logo 1\" width=\"250\"/>\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/3/3f/Qube_Research_%26_Technologies_Logo.svg/1200px-Qube_Research_%26_Technologies_Logo.svg.png\" alt=\"Logo 2\" width=\"200\" style=\"margin-left: 20px;\"/>\n",
    "</p> -->\n",
    "\n",
    "# Data Challenge : Leukemia Risk Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GOAL OF THE CHALLENGE and WHY IT IS IMPORTANT:*\n",
    "\n",
    "The goal of the challenge is to **predict disease risk for patients with blood cancer**, in the context of specific subtypes of adult myeloid leukemias.\n",
    "\n",
    "The risk is measured through the **overall survival** of patients, i.e. the duration of survival from the diagnosis of the blood cancer to the time of death or last follow-up.\n",
    "\n",
    "Estimating the prognosis of patients is critical for an optimal clinical management. \n",
    "For exemple, patients with low risk-disease will be offered supportive care to improve blood counts and quality of life, while patients with high-risk disease will be considered for hematopoietic stem cell transplantion.\n",
    "\n",
    "The performance metric used in the challenge is the **IPCW-C-Index**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "*THE DATASETS*\n",
    "\n",
    "The **training set is made of 3,323 patients**.\n",
    "\n",
    "The **test set is made of 1,193 patients**.\n",
    "\n",
    "For each patient, you have acces to CLINICAL data and MOLECULAR data.\n",
    "\n",
    "The details of the data are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OUTCOME:\n",
    "  * OS_YEARS = Overall survival time in years\n",
    "  * OS_STATUS = 1 (death) , 0 (alive at the last follow-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- CLINICAL DATA, with one line per patient:\n",
    "  \n",
    "  * ID = unique identifier per patient\n",
    "  * CENTER = clinical center\n",
    "  * BM_BLAST = Bone marrow blasts in % (blasts are abnormal blood cells)\n",
    "  * WBC = White Blood Cell count in Giga/L \n",
    "  * ANC = Absolute Neutrophil count in Giga/L\n",
    "  * MONOCYTES = Monocyte count in Giga/L\n",
    "  * HB = Hemoglobin in g/dL\n",
    "  * PLT = Platelets coutn in Giga/L\n",
    "  * CYTOGENETICS = A description of the karyotype observed in the blood cells of the patients, measured by a cytogeneticist. Cytogenetics is the science of chromosomes. A karyotype is performed from the blood tumoral cells. The convention for notation is ISCN (https://en.wikipedia.org/wiki/International_System_for_Human_Cytogenomic_Nomenclature). Cytogenetic notation are: https://en.wikipedia.org/wiki/Cytogenetic_notation. Note that a karyotype can be normal or abnornal. The notation 46,XX denotes a normal karyotype in females (23 pairs of chromosomes including 2 chromosomes X) and 46,XY in males (23 pairs of chromosomes inclusing 1 chromosme X and 1 chromsome Y). A common abnormality in the blood cancerous cells might be for exemple a loss of chromosome 7 (monosomy 7, or -7), which is typically asssociated with higher risk disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO side note : mesures sont dans des échelles différentes donc attention à normaliser et standardiser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- GENE MOLECULAR DATA, with one line per patient per somatic mutation. Mutations are detected from the sequencing of the blood tumoral cells. \n",
    "We call somatic (= acquired) mutations the mutations that are found in the tumoral cells but not in other cells of the body.\n",
    "\n",
    "  * ID = unique identifier per patient\n",
    "  * CHR START END = position of the mutation on the human genome\n",
    "  * REF ALT = reference and alternate (=mutant) nucleotide\n",
    "  * GENE = the affected gene\n",
    "  * PROTEIN_CHANGE = the consequence of the mutation on the protei that is expressed by a given gene\n",
    "  * EFFECT = a broad categorization of the mutation consequences on a given gene.\n",
    "  * VAF = Variant Allele Fraction = it represents the **proportion** of cells with the deleterious mutations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored , concordance_index_ipcw\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sksurv.util import Surv\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files in the directory\n",
    "# print(os.listdir(\".\"))\n",
    "\n",
    "# Clinical Data\n",
    "# TODO : adapting to my local path\n",
    "df = pd.read_csv(\"../data/X_train/clinical_train.csv\")\n",
    "df_eval = pd.read_csv(\"../data/X_test/clinical_test.csv\")\n",
    "\n",
    "# Molecular Data\n",
    "maf_df = pd.read_csv(\"../data/X_train/molecular_train.csv\")\n",
    "maf_eval = pd.read_csv(\"../data/X_test/molecular_test.csv\")\n",
    "\n",
    "target_df = pd.read_csv(\"../data/target_train.csv\")\n",
    "# TODO: sera fourni le 15 ou le 17 mars\n",
    "# target_df_test = pd.read_csv(\"./target_test.csv\")\n",
    "\n",
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation (clinical data only)\n",
    "\n",
    "For survival analysis, we’ll format the dataset so that OS_YEARS represents the time variable and OS_STATUS represents the event indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starting of Data Cleaning and Preprocessing\n",
    "\n",
    "# Drop rows where 'OS_YEARS' is NaN if conversion caused any issues\n",
    "# initial shape\n",
    "# print(target_df.shape)\n",
    "# drop rows with missing values\n",
    "target_df.dropna(subset=['OS_YEARS', 'OS_STATUS'], inplace=True)\n",
    "# final shape\n",
    "# print(target_df.shape)\n",
    "# percentage of rows dropped:\n",
    "print(f'Percentage of initially dropt rows {(1 - target_df.shape[0] / df.shape[0]) * 100:.2f}%')\n",
    "\n",
    "# Check the data types to ensure 'OS_STATUS' is boolean and 'OS_YEARS' is numeric\n",
    "print(target_df[['OS_STATUS', 'OS_YEARS']].dtypes)\n",
    "\n",
    "# Contarget_dfvert 'OS_YEARS' to numeric if it isn’t already\n",
    "target_df['OS_YEARS'] = pd.to_numeric(target_df['OS_YEARS'], errors='coerce')\n",
    "\n",
    "# Ensure 'OS_STATUS' is boolean\n",
    "target_df['OS_STATUS'] = target_df['OS_STATUS'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Selection :\n",
    "\n",
    "# Zero : features they selected for the Benchmark model :\n",
    "features_basic = ['BM_BLAST', 'HB', 'PLT']\n",
    "# Accuracies for each implemented model :\n",
    "# Benchmark LigthGBM: \n",
    "# Benchmark CoxPH:\n",
    "# Benchmark RandomSurvivalForest:\n",
    "# Benchmark when adding the Nmut count as a feature :\n",
    "\n",
    "\n",
    "# First: Naively add all the features  (except the Gene column):\n",
    "# features= ['BM_BLAST', 'HB', 'PLT', 'WBC', 'ANC', 'MONOCYTES']\n",
    "# Very naive : slight improvement\n",
    "\n",
    "# Second : construct some features based on scientific knowledge, and add them to the dataframe and the features list\n",
    "df['BLAST_per_WBC'] = df['BM_BLAST'] / (df['WBC'] + 1e-8)\n",
    "df['ANC_per_WBC'] = df['ANC'] / (df['WBC'] + 1e-8)\n",
    "# df['MONOCYTES_per_WBC'] = df['MONOCYTES'] / df['WBC']           # too much missing values, first drop it, then see how to exploit it\n",
    "df[\"PL_per_HB\"] = df['PLT'] / (df['HB'] + 1e-8)\n",
    "# add these features to the features list\n",
    "features_additional = ['BLAST_per_WBC', 'ANC_per_WBC', 'PL_per_HB']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la meme pour le df_eval\n",
    "df_eval['BLAST_per_WBC'] = df_eval['BM_BLAST'] / (df_eval['WBC'] + 1e-8)\n",
    "df_eval['ANC_per_WBC'] = df_eval['ANC'] / (df_eval['WBC'] + 1e-8)\n",
    "df_eval[\"PL_per_HB\"] = df_eval['PLT'] / (df_eval['HB'] + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third approach - exploiter les données cytogénétiques\n",
    "import re\n",
    "high_risk_patterns = [r\"-7\", r\"-5\", r\"-17\", r\"del\\(5\\)\", r\"del\\(17\\)\", r\"del\\(20\\)\", r\"del\\(9\\)\",\n",
    "                      r\"\\+8\", r\"t\\(3;3\\)\", r\"complex\", r\"t\\(9;11\\)\", r\"i\\(17\\)\", r\"inv\\(3\\)\"]\n",
    "\n",
    "def categorize_cytogenetics(cytogenetics):\n",
    "    if pd.isna(cytogenetics) or cytogenetics.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    cytogenetics = cytogenetics.upper()\n",
    "    for pattern in high_risk_patterns:\n",
    "        if re.search(pattern, cytogenetics):\n",
    "            return \"High_Risk\"\n",
    "    return \"Low_Intermediate\"\n",
    "\n",
    "\n",
    "####################################################\n",
    "# TODO : faire plus de catégories\n",
    "def enhanced_cytogenetics(cytogenetics):\n",
    "    if pd.isna(cytogenetics) or cytogenetics.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    count = 0\n",
    "    cytogenetics = cytogenetics.upper()\n",
    "    for pattern in high_risk_patterns:\n",
    "        if re.search(pattern, cytogenetics):\n",
    "            return \"High_Risk\"\n",
    "        # else if they are low intermediate : count them and if they are more than 3, then return high risk\n",
    "        else :\n",
    "            count += 1\n",
    "    if count >= 3 :\n",
    "        # TODO : ou plutot use the percentage to transform into one hot encoding?\n",
    "        return \"High_Risk\"\n",
    "    return \"Low_Intermediate\"\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "# Suggestion Chatgpt :\n",
    "# 3. Refined Cytogenetic Risk Categorization\n",
    "medium_risk_patterns = [r\"\\+21\", r\"\\+22\", r\"-Y\", r\"t\\(8;21\\)\", r\"t\\(15;17\\)\", r\"t\\(9;22\\)\"]\n",
    "\n",
    "def refined_categorize_cytogenetics(cytogenetics):\n",
    "    if pd.isna(cytogenetics) or cytogenetics.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    cytogenetics = cytogenetics.upper()\n",
    "    if any(re.search(pattern, cytogenetics) for pattern in high_risk_patterns):\n",
    "        return \"High_Risk\"\n",
    "    elif any(re.search(pattern, cytogenetics) for pattern in medium_risk_patterns):\n",
    "        return \"Medium_Risk\"\n",
    "    return \"Low_Intermediate\"\n",
    "\n",
    "df[\"CYTO_RISK\"] = df[\"CYTOGENETICS\"].apply(refined_categorize_cytogenetics)\n",
    "\n",
    "# One-hot encoding for risk categories\n",
    "df = pd.get_dummies(df, columns=[\"CYTO_RISK\"])\n",
    "\n",
    "# Drop original cytogenetics column\n",
    "df.drop(columns=[\"CYTOGENETICS\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CYTO_RISK\"] = df[\"CYTOGENETICS\"].apply(categorize_cytogenetics)\n",
    "# df[\"CYTO_RISK\"] = df[\"CYTOGENETICS\"].apply(enhanced_cytogenetics)\n",
    "df = pd.get_dummies(df, columns=[\"CYTO_RISK\"])\n",
    "# drop the \"Unknown\" column\n",
    "# est ce que ça suffit de garder la \"High risk column\" ?\n",
    "df.drop(columns=[\"CYTO_RISK_Unknown\"], inplace=True)\n",
    "df.drop(columns=[\"CYTO_RISK_Low_Intermediate\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cytogenetics = ['CYTO_RISK_High_Risk']\n",
    "df.drop(columns=[\"CYTOGENETICS\"], inplace=True)\n",
    "df.head()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meme transformation pour le df_eval\n",
    "df_eval[\"CYTO_RISK\"] = df_eval[\"CYTOGENETICS\"].apply(categorize_cytogenetics)\n",
    "df_eval = pd.get_dummies(df_eval, columns=[\"CYTO_RISK\"])\n",
    "df_eval.drop(columns=[\"CYTO_RISK_Unknown\"], inplace=True)\n",
    "df_eval.drop(columns=[\"CYTO_RISK_Low_Intermediate\"], inplace=True)\n",
    "df_eval.drop(columns=[\"CYTOGENETICS\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_eval.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.A : Process Molecular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load molecular data\n",
    "mutation_data = maf_df.copy()  # Assuming maf_df contains molecular variants\n",
    "\n",
    "# One-hot encode gene mutations\n",
    "mutation_features = mutation_data.pivot_table(index='ID', columns='GENE', values='VAF', aggfunc='max').fillna(0)\n",
    "\n",
    "# One-hot encode mutation effects\n",
    "mutation_effects = pd.get_dummies(mutation_data[['ID', 'EFFECT']], columns=['EFFECT']).groupby('ID').sum()\n",
    "\n",
    "# Update feature list\n",
    "molecular_features = list(mutation_features.columns)\n",
    "molecular_effect = list(mutation_effects.columns)\n",
    "# molecular_package = molecular_features + molecular_effect\n",
    "# molecular_features = list(mutation_features.columns) + list(mutation_effects.columns)\n",
    "print(df.shape)\n",
    "print(mutation_effects.shape)\n",
    "print(mutation_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same to the eval set\n",
    "mutation_data_eval = maf_eval.copy()\n",
    "mutation_features_eval = mutation_data_eval.pivot_table(index='ID', columns='GENE', values='VAF', aggfunc='max').fillna(0)\n",
    "mutation_effects_eval = pd.get_dummies(mutation_data_eval[['ID', 'EFFECT']], columns=['EFFECT']).groupby('ID').sum()\n",
    "\n",
    "# define the list of molecular features of the eval set\n",
    "molecular_features_eval = list(mutation_features_eval.columns)\n",
    "molecular_effect_eval = list(mutation_effects_eval.columns)\n",
    "# molecular_package_eval = molecular_features_eval + molecular_effect_eval\n",
    "print(df_eval.shape)\n",
    "print(mutation_effects_eval.shape)\n",
    "print(mutation_features_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to ge the common molecular features between the train and the eval set\n",
    "common_molecular_features = set(molecular_features).intersection(set(molecular_features_eval))\n",
    "print(len(common_molecular_features))\n",
    "\n",
    "# # now I want to merge df to maf_df and df_eval to maf_evaln only on the common features\n",
    "# print(list(common_molecular_features))\n",
    "# print(mutation_features.columns)\n",
    "df = df.merge(mutation_features[list(common_molecular_features)], on='ID', how='left').fillna(0)\n",
    "df_eval = df_eval.merge(mutation_features_eval[list(common_molecular_features)], on='ID', how='left').fillna(0)\n",
    "\n",
    "common_molecular_effect = set(molecular_effect).intersection(set(molecular_effect_eval))\n",
    "print(len(common_molecular_effect))\n",
    "df = df.merge(mutation_effects[list(common_molecular_effect)], on='ID', how='left').fillna(0)\n",
    "df_eval = df_eval.merge(mutation_effects_eval[list(common_molecular_effect)], on='ID', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_molecular_info = list(common_molecular_features) + list(common_molecular_effect)\n",
    "features = features_basic + features_additional + features_cytogenetics + list_molecular_info\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : est ce que any csv has been affected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = features_basic\n",
    "# features = features_basic + features_additional\n",
    "\n",
    "# features = features_basic + features_additional + features_cytogenetics\n",
    "print(\"The features are:\", features)\n",
    "target = ['OS_YEARS', 'OS_STATUS']\n",
    "print(len(features))\n",
    "\n",
    "# print(len(molecular_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : I can't take the common colums of the molecular features because they are not in the eval set (mais my slip is deterministic.... comment remedier à ça?)\n",
    "\n",
    "# features += molecular_features\n",
    "# print(\"Updated feature set with molecular data:\", features)\n",
    "# print(\"Number of features:\", len(features))   # 147 features ------- est ce que c'est abbérant % nombre de data points?\n",
    "\n",
    "# # Take the list of common columns as the features\n",
    "# features = list(common_columns)\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the survival data format\n",
    "X = df.loc[df['ID'].isin(target_df['ID']), features]\n",
    "y = Surv.from_dataframe('OS_STATUS', 'OS_YEARS', target_df)\n",
    "\n",
    "print(target_df[['OS_STATUS', 'OS_YEARS']].dtypes)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO : try the CoxPHFitter\n",
    "# # Select top 50 best features\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_regression\n",
    "# selector = SelectKBest(score_func=f_regression, k=50)\n",
    "# X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : 0 quand on a pas les données -- c'est chaud \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Splitting the Dataset\n",
    "We’ll split the data into training and testing sets to evaluate the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage of missing values per column\n",
    "missing_percentage = X_train.isnull().mean() * 100\n",
    "# Print the missing values percentage (genre de manière générale pas selon les catégories)\n",
    "print(missing_percentage)\n",
    "\n",
    "\n",
    "# TODO : idk if we can do an imputation for the molecular data?????\n",
    "\n",
    "# Survival-aware imputation for missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[features] = imputer.fit_transform(X_train[features])\n",
    "X_test[features] = imputer.transform(X_test[features])\n",
    "\n",
    "# TODO : est ce que faut faire chaque impuration pour chaque catégorie de features seules?\n",
    "\n",
    "# X_train[features_cytogenetics] = imputer.fit_transform(X_train[features_cytogenetics])\n",
    "# X_test[features_cytogenetics] = imputer.transform(X_test[features_cytogenetics])\n",
    "\n",
    "# # do the same for the additional features\n",
    "# X_train[features_additional] = imputer.fit_transform(X_train[features_additional])\n",
    "# X_test[features_additional] = imputer.transform(X_test[features_additional])\n",
    "\n",
    "# Attention additional features have more missing values : \n",
    "# WBC           7.3 %\n",
    "# ANC           4.6 %\n",
    "# MONOCYTES    16.3 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : maybe drop les datas ou ya bcp de manque??\n",
    "\n",
    "# features.remove('PL_per_HB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training Standard Machine Learning Methods\n",
    "\n",
    "In this step, we train a standard LightGBM model on survival data, but we do not account for censoring. Instead of treating the event status, we use only the observed survival times as the target variable. This approach disregards whether an individual’s event (e.g., death) was observed or censored, effectively treating the problem as a standard regression task. While this method provides a basic benchmark, it may be less accurate than survival-specific models (but still be explored!), as it does not leverage the information contained in censored observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# Define LightGBM parameters\n",
    "lgbm_params = {\n",
    "    'max_depth': 3,         # TODO : when adding features try to increase the depth of the tree\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': -1,\n",
    "    # 'alpha': 100,\n",
    "}\n",
    "\n",
    "# Prepare the data for LightGBM\n",
    "# Scale the target (OS_YEARS) to reduce skew, apply weights based on event status\n",
    "X_train_lgb = X_train  # Features for training\n",
    "y_train_transformed = y_train['OS_YEARS']\n",
    "\n",
    "# Create LightGBM dataset\n",
    "train_dataset = lgb.Dataset(X_train_lgb, label=y_train_transformed)\n",
    "\n",
    "# Train the LightGBM model\n",
    "model = lgb.train(params=lgbm_params, train_set=train_dataset)\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "pred_train = -model.predict(X_train)\n",
    "pred_test = -model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Concordance Index IPCW\n",
    "train_ci_ipcw = concordance_index_ipcw(y_train, y_train, pred_train, tau=7)[0]\n",
    "test_ci_ipcw = concordance_index_ipcw(y_train, y_test, pred_test, tau=7)[0]\n",
    "print(f\"LightGBM Survival Model Concordance Index IPCW on train: {train_ci_ipcw:.2f}\")\n",
    "print(f\"LightGBM Survival Model Concordance Index IPCW on test: {test_ci_ipcw:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a standard XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a XGBoost model for survival analysis\n",
    "xgb_params = {\n",
    "    \"objective\": \"rank:pairwise\",  # Optimized for ranking risk scores\n",
    "    \"eval_metric\": \"ndcg\",  # Normalized Discounted Cumulative Gain\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbosity': 0,\n",
    "}\n",
    "\n",
    "# Prepare the data for XGBoost\n",
    "X_train_xgb = X_train\n",
    "y_train_xgb = y_train['OS_YEARS']\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=100)\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "dtest_train = xgb.DMatrix(X_train)\n",
    "dtest_test = xgb.DMatrix(X_test)\n",
    "pred_train_xgb = -xgb_model.predict(dtest_train)\n",
    "pred_test_xgb = -xgb_model.predict(dtest_test)\n",
    "\n",
    "# Evaluate the model using Concordance Index IPCW\n",
    "train_ci_ipcw_xgb = concordance_index_ipcw(y_train, y_train, pred_train_xgb, tau=7)[0]\n",
    "test_ci_ipcw_xgb = concordance_index_ipcw(y_train, y_test, pred_test_xgb, tau=7)[0]\n",
    "print(f\"XGBoost Survival Model Concordance Index IPCW on train: {train_ci_ipcw_xgb:.2f}\")\n",
    "print(f\"XGBoost Survival Model Concordance Index IPCW on test: {test_ci_ipcw_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'verbosity': [0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Initialize GridSearchCV with the XGBoost model and parameter grid\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "# TODO : il fait un grid search avec un negative mean squared error\n",
    "\n",
    "# Prepare the data for XGBoost\n",
    "X_train_xgb = X_train\n",
    "y_train_xgb = y_train['OS_YEARS']\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best score found: {best_score}\")\n",
    "\n",
    "# Train the XGBoost model with the best parameters\n",
    "best_xgb_model = xgb.XGBRegressor(**best_params)\n",
    "best_xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "dtest_train = xgb.DMatrix(X_train)\n",
    "dtest_test = xgb.DMatrix(X_test)\n",
    "pred_train_xgb = -best_xgb_model.predict(dtest_train)\n",
    "pred_test_xgb = -best_xgb_model.predict(dtest_test)\n",
    "\n",
    "# Evaluate the model using Concordance Index IPCW\n",
    "train_ci_ipcw_xgb = concordance_index_ipcw(y_train, y_train, pred_train_xgb, tau=7)[0]\n",
    "test_ci_ipcw_xgb = concordance_index_ipcw(y_train, y_test, pred_test_xgb, tau=7)[0]\n",
    "print(f\"XGBoost Survival Model Concordance Index IPCW on train: {train_ci_ipcw_xgb:.2f}\")\n",
    "print(f\"XGBoost Survival Model Concordance Index IPCW on test: {test_ci_ipcw_xgb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : save submission file of the lgb method\n",
    "# TODO : train light gbm with the number of mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \n",
    "\n",
    "- le modèle commence à overfitter sur le train set quand on ajoute des features (ratio des PL et HB et ratios similaires, et aussi cytogenetic data)\n",
    "- il a encore plus overfitté sur le train avec l'ajout des data moléculaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZING THE \n",
    "# \n",
    "# # Assuming the LightGBM model is defined as `model`\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# lgb.plot_tree(model, tree_index=0, figsize=(20, 10), show_info=['split_gain', 'internal_value', 'internal_count', 'leaf_count'])\n",
    "# plt.title(\"First Tree in LightGBM Model\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Cox Proportional Hazards Model\n",
    "\n",
    "To account for censoring in survival analysis, we use a Cox Proportional Hazards (Cox PH) model, a widely used method that estimates the effect of covariates on survival times without assuming a specific baseline survival distribution. The Cox PH model is based on the hazard function, $h(t | X)$, which represents the instantaneous risk of an event (e.g., death) at time $t$ given covariates $X$. The model assumes that the hazard can be expressed as:\n",
    "\n",
    "$$h(t | X) = h_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p)$$\n",
    "\n",
    "\n",
    "where $h_0(t)$ is the baseline hazard function, and $\\beta$ values are coefficients for each covariate, representing the effect of $X$ on the hazard. Importantly, the proportional hazards assumption implies that the hazard ratios between individuals are constant over time. This approach effectively leverages both observed and censored survival times, making it a more suitable method for survival data compared to standard regression techniques that ignore censoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Cox Proportional Hazards model\n",
    "cox = CoxPHSurvivalAnalysis(alpha=10.0)\n",
    "# TODO : ajuster les hyperparamètres\n",
    "cox.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using Concordance Index IPCW\n",
    "cox_cindex_train = concordance_index_ipcw(y_train, y_train, cox.predict(X_train), tau=7)[0]\n",
    "cox_cindex_test = concordance_index_ipcw(y_train, y_test, cox.predict(X_test), tau=7)[0]\n",
    "print(f\"Cox Proportional Hazard Model Concordance Index IPCW on train: {cox_cindex_train:.2f}\")\n",
    "print(f\"Cox Proportional Hazard Model Concordance Index IPCW on test: {cox_cindex_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and train the Cox Proportional Hazards model\n",
    "# from lifelines import CoxPHFitter\n",
    "# cox2 = CoxPHFitter()\n",
    "# # TODO : ajuster les hyperparamètres\n",
    "# cox2.fit(df, duration_col='OS_YEARS', event_col='OS_STATUS')\n",
    "# # fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model using Concordance Index IPCW\n",
    "# cox2_cindex_train = concordance_index_ipcw(y_train, y_train, cox2.predict(X_train), tau=7)[0]\n",
    "# cox2_cindex_test = concordance_index_ipcw(y_train, y_test, cox2.predict(X_test), tau=7)[0]\n",
    "# print(f\"Cox Proportional Hazard Model Concordance Index IPCW on train: {cox2_cindex_train:.2f}\")\n",
    "# print(f\"Cox Proportional Hazard Model Concordance Index IPCW on test: {cox2_cindex_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING suggested by chatgpt quand on prend en compte les mutations data :\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = X_train.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "# plt.show()\n",
    "\n",
    "# TODO : ya bcp de correlation entre certains features!!!!! dans le petit carré en haut à gauche ----- faire de la feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a threshold of correlation and print all the pairs of features that are highly correlated\n",
    "threshold = 0.7\n",
    "print(len(features))\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        if corr_matrix.iloc[i, j] > threshold:\n",
    "            print(f\"Features {features[i]} and {features[j]} are highly correlated with a correlation of {corr_matrix.iloc[i, j]:.2f}\")\n",
    "\n",
    "# print(\"3rd and sixth features are highly correlated\")\n",
    "# print(corr_matrix.iloc[2, 5])\n",
    "# print(\"those features are :\")\n",
    "# print(features[2],\"and\", features[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the pairs of highly correlated features, we can drop one of them\n",
    "# Drop the highly correlated features\n",
    "features.remove('PL_per_HB')\n",
    "\n",
    "# update the training and testing sets\n",
    "X_train = X_train.drop(columns=['PL_per_HB'])\n",
    "X_test = X_test.drop(columns=['PL_per_HB'])\n",
    "\n",
    "X = X.drop(columns=['PL_per_HB'])\n",
    "\n",
    "print(len(features))\n",
    "\n",
    "\n",
    "# recompute the correlation matrix\n",
    "corr_matrix = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "# plt.title(\"Feature Correlation Matrix\")\n",
    "\n",
    "threshold = 0.7\n",
    "print(len(features))\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        if corr_matrix.iloc[i, j] > threshold:\n",
    "            print(f\"Features {features[i]} and {features[j]} are highly correlated with a correlation of {corr_matrix.iloc[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Naive Approach to Incorporate Mutations\n",
    "\n",
    "In this step, we take a very naive approach to account for genetic mutations by simply counting the total number of somatic mutations per patient. Instead of analyzing specific mutations or their biological impact, we use this aggregate count as a basic feature to reflect the mutational burden for each individual. Although simplistic, this feature can serve as a general indicator of genetic variability across patients, which may influence survival outcomes. More sophisticated mutation analysis could be incorporated in future models to improve predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Extract the number of somatic mutations per patient\n",
    "# Group by 'ID' and count the number of mutations (rows) per patient\n",
    "tmp = maf_df.groupby('ID').size().reset_index(name='Nmut')\n",
    "\n",
    "# Merge with the training dataset and replace missing values in 'Nmut' with 0\n",
    "df_2 = df.merge(tmp, on='ID', how='left').fillna({'Nmut': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "# features = ['BM_BLAST', 'HB', 'PLT', 'Nmut']\n",
    "# features = ['BM_BLAST', 'HB', 'PLT', 'WBC', 'ANC', 'MONOCYTES', 'Nmut']\n",
    "# features = features_basic + ['BLAST_per_WBC', 'ANC_per_WBC', 'PL_per_HB', 'Nmut']\n",
    "# features = features_basic + ['Nmut']\n",
    "features = features + ['Nmut']\n",
    "target = ['OS_YEARS', 'OS_STATUS']\n",
    "\n",
    "# Create the survival data format\n",
    "X = df_2.loc[df_2['ID'].isin(target_df['ID']), features]\n",
    "y = Surv.from_dataframe('OS_STATUS', 'OS_YEARS', target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# TODO : change the random state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival-aware imputation for missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[features] = imputer.fit_transform(X_train[features])\n",
    "X_test[features] = imputer.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the features are:\", features)\n",
    "print(\"the number of features:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Cox Proportional Hazards model\n",
    "# cox = CoxPHSurvivalAnalysis()\n",
    "cox = CoxPHSurvivalAnalysis(alpha=10.)\n",
    "# TODO : ajuster les hyperparamètres\n",
    "cox.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using Concordance Index IPCW\n",
    "cox_cindex_train = concordance_index_ipcw(y_train, y_train, cox.predict(X_train), tau=7)[0]\n",
    "cox_cindex_test = concordance_index_ipcw(y_train, y_test, cox.predict(X_test), tau=7)[0]\n",
    "print(f\"Cox Proportional Hazard Model Concordance Index IPCW on train: {cox_cindex_train:.2f}\")\n",
    "print(f\"Cox Proportional Hazard Model Concordance Index IPCW on test: {cox_cindex_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Random Survival Forest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selecting the adequate features for the Random Survival Forest model\n",
    "# features = features_basic + features_additional + features_cytogenetics + ['Nmut']\n",
    "# or just using all the features\n",
    "\n",
    "# # Survival-aware imputation for missing values\n",
    "# imputer = SimpleImputer(strategy=\"median\")\n",
    "# X_train[features] = imputer.fit_transform(X_train[features])\n",
    "# X_test[features] = imputer.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Survival Forest\n",
    "rsf = RandomSurvivalForest(n_estimators=200, min_samples_split=10, min_samples_leaf=5, max_depth=5, random_state=42)\n",
    "rsf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate using IPCW Concordance Index\n",
    "rsf_cindex_train = concordance_index_ipcw(y_train, y_train, rsf.predict(X_train), tau=7)[0]\n",
    "rsf_cindex_test = concordance_index_ipcw(y_train, y_test, rsf.predict(X_test), tau=7)[0]\n",
    "\n",
    "print(f\"RSF Concordance Index IPCW on train: {rsf_cindex_train:.2f}\")\n",
    "print(f\"RSF Concordance Index IPCW on test: {rsf_cindex_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for a different horizon : TAU = 5 years\n",
    "\n",
    "# Initialize and train Random Survival Forest\n",
    "rsf2 = RandomSurvivalForest(n_estimators=200, min_samples_split=10, min_samples_leaf=5, max_depth=5, random_state=42)\n",
    "rsf2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate using IPCW Concordance Index\n",
    "rsf2_cindex_train = concordance_index_ipcw(y_train, y_train, rsf2.predict(X_train), tau=5)[0]\n",
    "rsf2_cindex_test = concordance_index_ipcw(y_train, y_test, rsf2.predict(X_test), tau=5)[0]\n",
    "\n",
    "print(f\"RSF Concordance Index IPCW on train: {rsf2_cindex_train:.2f}\")\n",
    "print(f\"RSF Concordance Index IPCW on test: {rsf2_cindex_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best parameters found: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "# # Initialize and train Random Survival Forest\n",
    "# wow = RandomSurvivalForest(n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_depth=3, random_state=42)\n",
    "# wow.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate using IPCW Concordance Index\n",
    "# wow_cindex_train = concordance_index_ipcw(y_train, y_train, wow.predict(X_train), tau=7)[0]\n",
    "# wow_cindex_test = concordance_index_ipcw(y_train, y_test, wow.predict(X_test), tau=7)[0]\n",
    "\n",
    "# print(f\"RSF Concordance Index IPCW on train: {wow_cindex_train:.2f}\")\n",
    "# print(f\"RSF Concordance Index IPCW on test: {wow_cindex_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "# TODO : maybe mix long and short term predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for the best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid for the Random Survival Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize the Random Survival Forest model\n",
    "rsf = RandomSurvivalForest(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the RSF model and parameter grid\n",
    "grid_search = GridSearchCV(estimator=rsf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best score found: {best_score}\")\n",
    "\n",
    "# TODO : faire un grid search pour toutes les méthodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "### Inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_eval = maf_eval.groupby('ID').size().reset_index(name='Nmut')\n",
    "\n",
    "# Merge with the training dataset and replace missing values in 'Nmut' with 0\n",
    "df_eval = df_eval.merge(tmp_eval, on='ID', how='left').fillna({'Nmut': 0})\n",
    "\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval.drop(columns=\"Nmut_x\", inplace=True)    \n",
    "# df_eval.rename(columns={\"Nmut_y\": \"Nmut\"}, inplace=True)\n",
    "# df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the features we have selected\n",
    "# print(features)\n",
    "# df_eval.head()\n",
    "df_eval[features] = imputer.transform(df_eval[features])\n",
    "cox_prediction_on_test_set = cox.predict(df_eval.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval[features] = imputer.transform(df_eval[features])\n",
    "rsf_prediction_on_test_set = rsf.predict(df_eval.loc[:, features])\n",
    "# lgb_prediction_on_test_set = model.predict(df_eval.loc[:, features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.Series(prediction_on_test_set, index=df_eval['ID'], name='OS_YEARS')\n",
    "cox_test_submission = pd.Series(cox_prediction_on_test_set, index=df_eval['ID'], name='risk_score')\n",
    "rsf_test_submission = pd.Series(rsf_prediction_on_test_set, index=df_eval['ID'], name='risk_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "os.makedirs('./output', exist_ok=True)\n",
    "\n",
    "# I just want the date and hour and minute\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# submission.to_csv(f'./output/submission_{now}_rsf_ok.csv')\n",
    "cox_test_submission.to_csv(f'./output/submission_{now}_cox.csv')\n",
    "rsf_test_submission.to_csv(f'./output/submission_{now}_rsf.csv')\n",
    "# submission.to_csv('./output/all_clinical_features_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "# cox_test_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def now_str():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d_%H-%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "### Stacking a meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load test survival data\n",
    "# rsf_preds = pd.read_csv(\"./output/submission_2025-03-10_00-29_rsf.csv\", index_col=\"ID\")  \n",
    "# cox_preds = pd.read_csv(\"./output/submission_2025-03-10_00-29_cox.csv\", index_col=\"ID\")  \n",
    "\n",
    "# Stack RSF & Cox risk scores\n",
    "stacked_X_train = pd.DataFrame({\"rsf_score\": rsf.predict(X_train), \"cox_score\": cox.predict(X_train)}, index=X_train.index)\n",
    "# stacked_X_test = pd.DataFrame({\"rsf_score\": rsf_preds[\"risk_score\"], \"cox_score\": cox_preds[\"risk_score\"]})\n",
    "stacked_X_test = pd.DataFrame({\"rsf_score\": rsf.predict(X_test), \"cox_score\": cox.predict(X_test)}, index=X_test.index)\n",
    "\n",
    "# Ensure that y_train and stacked_X_train have the same shape\n",
    "# assert stacked_X_train.shape[0] == y_train.shape[0], \"Mismatched train data sizes!\"\n",
    "\n",
    "# Train a meta-learner\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_X_train, y_train)\n",
    "\n",
    "# Predict combined survival scores\n",
    "ensemble_preds = meta_model.predict_proba(stacked_X_test)[:, 1]\n",
    "\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")\n",
    "# print(f\"ensemble_preds shape: {ensemble_preds.shape}\")\n",
    "\n",
    "# Evaluate IPCW C-index\n",
    "ensemble_cindex = concordance_index_ipcw(y_train, y_test, ensemble_preds, tau=7)[0]\n",
    "print(f\"Meta-Learner IPCW C-index: {ensemble_cindex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "# print(y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Z norm maybe before applying the meta model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "\n",
    "# Stack RSF & Cox risk scores\n",
    "stacked_X_train = pd.DataFrame({\"rsf_score\": rsf.predict(X_train), \"cox_score\": cox.predict(X_train)}, index=X_train.index)\n",
    "stacked_X_test = pd.DataFrame({\"rsf_score\": rsf.predict(X_test), \"cox_score\": cox.predict(X_test)}, index=X_test.index)\n",
    "\n",
    "# Convert survival labels to a format usable by XGBoost (duration and event)\n",
    "y_train_event = y_train[\"OS_STATUS\"].astype(int)  # 1 if event occurred, 0 otherwise\n",
    "y_train_time = y_train[\"OS_YEARS\"]  # Survival time\n",
    "\n",
    "# Train an XGBoost model for survival ranking\n",
    "dtrain = xgb.DMatrix(stacked_X_train, label=y_train_time)  # Use time as label for ranking\n",
    "dtest = xgb.DMatrix(stacked_X_test)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"rank:pairwise\",  # Optimized for ranking risk scores\n",
    "    \"eval_metric\": \"ndcg\",  # Normalized Discounted Cumulative Gain\n",
    "    \"eta\": 0.05,  # Learning rate\n",
    "    \"max_depth\": 3,  # Small depth to avoid overfitting\n",
    "    \"subsample\": 0.8,  # Use 80% of data per tree\n",
    "    \"colsample_bytree\": 0.8,  # Use 80% of features per tree\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict survival risk scores\n",
    "train_ensemble_preds = -xgb_model.predict(dtrain)\n",
    "ensemble_preds = -xgb_model.predict(dtest)\n",
    "\n",
    "# Evaluate IPCW C-index\n",
    "# train ipcw c-index\n",
    "ensemble_cindex_train = concordance_index_ipcw(y_train, y_train, train_ensemble_preds, tau=7)[0]\n",
    "# test ipcw c-index\n",
    "ensemble_cindex = concordance_index_ipcw(y_train, y_test, ensemble_preds, tau=7)[0]\n",
    "print(f\"Optimized Meta-Learner IPCW C-index on train: {ensemble_cindex_train}\")\n",
    "print(f\"Optimized Meta-Learner IPCW C-index on test : {ensemble_cindex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacked_X_train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time\n",
    "\n",
    "# Load test survival data\n",
    "rsf_preds = pd.read_csv(\"./output/submission_2025-03-10_00-29_rsf.csv\", index_col=\"ID\")\n",
    "cox_preds = pd.read_csv(\"./output/submission_2025-03-10_00-29_cox.csv\", index_col=\"ID\")  \n",
    "\n",
    "# Stack RSF & Cox risk scores of the test set\n",
    "stacked_inference = pd.DataFrame({\"rsf_score\": rsf_preds[\"risk_score\"], \"cox_score\": cox_preds[\"risk_score\"]})\n",
    "dinference = xgb.DMatrix(stacked_inference)\n",
    "\n",
    "# Predict survival risk scores\n",
    "inference_preds = -xgb_model.predict(dinference)\n",
    "\n",
    "# Save the submission file\n",
    "submission = pd.Series(inference_preds, index=stacked_inference.index, name='risk_score')\n",
    "submission\n",
    "submission.to_csv(f'./output/submission_meta_xgboost_{now_str()}_ensemble.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************\n",
    "### Step 7 : try combining the 2 models' (COX, RSF) results :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('./output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Load RSF and Cox predictions\n",
    "# rsf_preds = pd.read_csv(\"./output/submission_2025-03-07_23-11_rsf.csv\", index_col=\"ID\")  \n",
    "# cox_preds = pd.read_csv(\"./output/submission_2025-03-07_20-34_cox.csv\", index_col=\"ID\")  \n",
    "\n",
    "# # Step 1: Convert Cox risk scores to survival time estimates\n",
    "# cox_preds[\"survival_time\"] = np.exp(-cox_preds[\"risk_score\"])  # Transforming risk scores\n",
    "# # print(cox_preds.head())\n",
    "\n",
    "# # Step 2: Normalize predictions (Min-Max Scaling)\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# rsf_preds[\"normalized\"] = scaler.fit_transform(rsf_preds.iloc[:, 0].values.reshape(-1, 1))\n",
    "# cox_preds[\"normalized\"] = scaler.fit_transform(cox_preds[\"survival_time\"].values.reshape(-1, 1))\n",
    "# # print(rsf_preds.head())\n",
    "# # print(cox_preds.head())\n",
    "\n",
    "# # Step 3: Ensemble (Weighted Average)\n",
    "# rsf_weight = 0.6  # Adjust based on validation set performance\n",
    "# cox_weight = 0.4\n",
    "\n",
    "# ensemble_preds = (rsf_weight * rsf_preds[\"normalized\"]) + (cox_weight * cox_preds[\"normalized\"])\n",
    "# ensemble_preds = pd.DataFrame(ensemble_preds, columns=[\"normalized\"], index=rsf_preds.index)\n",
    "# ensemble_preds.index.name = \"ID\"\n",
    "# # print(ensemble_preds.head())\n",
    "\n",
    "# # # Save final predictions\n",
    "# ensemble_preds.to_csv(\"./output/ensemble_predictions.csv\")\n",
    "\n",
    "# # print(\"Ensemble predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load RSF and Cox predictions\n",
    "rsf_preds = pd.read_csv(\"./output/submission_2025-03-07_23-11_rsf.csv\", index_col=\"ID\")  \n",
    "cox_preds = pd.read_csv(\"./output/submission_2025-03-07_20-34_cox.csv\", index_col=\"ID\") \n",
    "\n",
    "# Step 1: Convert RSF survival time into risk scores\n",
    "epsilon = 1e-6  # Small value to avoid division by zero\n",
    "rsf_preds[\"risk_score\"] = 1 / (rsf_preds.iloc[:, 0] + epsilon)\n",
    "\n",
    "# Step 2: Normalize both RSF and Cox risk scores\n",
    "scaler = MinMaxScaler()\n",
    "rsf_preds[\"normalized\"] = scaler.fit_transform(rsf_preds[\"risk_score\"].values.reshape(-1, 1))\n",
    "cox_preds[\"normalized\"] = scaler.fit_transform(cox_preds[\"risk_score\"].values.reshape(-1, 1))\n",
    "# print(rsf_preds.head())\n",
    "# print(cox_preds.head())\n",
    "\n",
    "# Step 3: Ensemble (Weighted Average)\n",
    "# rsf_weight = 0.5  # Adjust based on validation set performance\n",
    "# cox_weight = 0.5\n",
    "# Assign weights based on IPCW C-index\n",
    "rsf_weight = rsf_cindex_test / (rsf_cindex_test + cox_cindex_test)\n",
    "cox_weight = cox_cindex_test / (rsf_cindex_test + cox_cindex_test)\n",
    "\n",
    "print(f\"Optimized Weights -> RSF: {rsf_weight:.3f}, Cox: {cox_weight:.3f}\")\n",
    "\n",
    "\n",
    "ensemble_risk_scores = (rsf_weight * rsf_preds[\"normalized\"]) + (cox_weight * cox_preds[\"normalized\"])\n",
    "# format the ensemble risk scores to be a csv with the ID as the index, and the risk score as the only column\n",
    "ensemble_risk_scores = pd.DataFrame(ensemble_risk_scores, columns=[\"normalized\"], index=rsf_preds.index)\n",
    "ensemble_risk_scores.index.name = \"ID\"\n",
    "# rename the column to \"risk_score\"\n",
    "ensemble_risk_scores.rename(columns={\"normalized\": \"risk_score\"}, inplace=True)\n",
    "print(ensemble_risk_scores.head())\n",
    "\n",
    "\n",
    "# Save final predictions\n",
    "# ensemble_risk_scores.to_csv(\"ensemble_risk_scores.csv\")\n",
    "\n",
    "print(\"Ensemble risk scores saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : test chaque methode seule deja sur la platforme\n",
    "# - test les resultats de RSF, mais need to be formatted before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying to fix the ensemble issue :\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from scipy.stats import rankdata\n",
    "\n",
    "# # Load RSF and Cox predictions\n",
    "# rsf_preds = pd.read_csv(\"./output/submission_2025-03-07_23-11_rsf.csv\", index_col=\"ID\")  \n",
    "# cox_preds = pd.read_csv(\"./output/submission_2025-03-07_20-34_cox.csv\", index_col=\"ID\") \n",
    "\n",
    "# # Step 1: Convert RSF survival time into risk scores\n",
    "# epsilon = 1e-6  # Avoid division by zero\n",
    "# rsf_preds[\"risk_score\"] = 1 / (rsf_preds.iloc[:, 0] + epsilon)  # Inverting survival time\n",
    "\n",
    "# # Step 2: Compute IPCW C-index for both models\n",
    "# # Step 3: Assign weights dynamically\n",
    "# rsf_weight = rsf_cindex_test / (rsf_cindex_test + cox_cindex_test)\n",
    "# cox_weight = cox_cindex_test / (rsf_cindex_test + cox_cindex_test)\n",
    "\n",
    "# print(f\"Optimized Weights -> RSF: {rsf_weight:.3f}, Cox: {cox_weight:.3f}\")\n",
    "\n",
    "# # Step 4: Normalize using Rank Transformation\n",
    "# rsf_preds[\"ranked\"] = rankdata(rsf_preds[\"risk_score\"]) / len(rsf_preds)\n",
    "# cox_preds[\"ranked\"] = rankdata(cox_preds[\"risk_score\"]) / len(cox_preds)\n",
    "\n",
    "# # Step 5: Compute Weighted Ensemble\n",
    "# ensemble_risk_scores = (rsf_weight * rsf_preds[\"ranked\"]) + (cox_weight * cox_preds[\"ranked\"])\n",
    "# # print(ensemble_risk_scores)\n",
    "\n",
    "# # Convert back to original scale if needed (optional)\n",
    "# ensemble_risk_scores = pd.DataFrame(ensemble_risk_scores, columns=[\"ranked\"], index=rsf_preds.index)\n",
    "# ensemble_risk_scores.index.name = \"ID\"\n",
    "# ensemble_risk_scores.rename(columns={\"ranked\": \"risk_score\"}, inplace=True)\n",
    "# print(ensemble_risk_scores.head())\n",
    "\n",
    "# # Save final predictions in correct format\n",
    "# ensemble_risk_scores.to_csv(\"ensemble_risk_scores_reranked.csv\")\n",
    "\n",
    "# print(\"Final ensemble predictions saved with corrected risk alignment and weights!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to combine, COX and RSF model : classic linear averaging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load RSF and Cox predictions\n",
    "rsf_preds = pd.read_csv(\"./output/submission_2025-03-08_23-44_rsf_ok.csv\", index_col=\"ID\")  \n",
    "cox_preds = pd.read_csv(\"./output/submission_2025-03-07_20-34_cox.csv\", index_col=\"ID\") \n",
    "\n",
    "# # Step 1: Convert RSF survival time into risk scores\n",
    "# epsilon = 1e-6  # Small value to avoid division by zero\n",
    "# rsf_preds[\"risk_score\"] = 1 / (rsf_preds.iloc[:, 0] + epsilon)\n",
    "\n",
    "# I want to scale the rsf_preds[\"risk_score\"] to be between the minimum and maximum value of cox_preds\n",
    "# Step 2: Normalize RSF risk scores to the range of Cox risk scores\n",
    "min_cox = cox_preds[\"risk_score\"].min()\n",
    "max_cox = cox_preds[\"risk_score\"].max()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(min_cox, max_cox))\n",
    "rsf_preds[\"scaled_risk_score\"] = scaler.fit_transform(rsf_preds[[\"risk_score\"]])\n",
    "\n",
    "# Step 3: Combine the normalized RSF and Cox risk scores (e.g., averaging)\n",
    "combined_risk_score = (rsf_preds[\"scaled_risk_score\"] + cox_preds[\"risk_score\"]) / 2\n",
    "\n",
    "# Store final risk scores\n",
    "final_preds = pd.DataFrame({\"ID\": rsf_preds.index, \"risk_score\": combined_risk_score})\n",
    "\n",
    "# Save to CSV\n",
    "# final_preds.to_csv(\"./output/combined_risk_scores.csv\", index=False)\n",
    "# final_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING A NEW STRATEGY TO COMBINE BOTH RESULTS :\n",
    "import datetime\n",
    "\n",
    "os.makedirs('./output', exist_ok=True)\n",
    "\n",
    "# I just want the date and hour and minute\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# Load RSF and Cox predictions\n",
    "rsf_preds = pd.read_csv(\"./output/submission_2025-03-08_23-44_rsf_ok.csv\", index_col=\"ID\")  \n",
    "cox_preds = pd.read_csv(\"./output/submission_2025-03-07_20-34_cox.csv\", index_col=\"ID\")\n",
    "\n",
    "# Standardize RSF and Cox risk scores\n",
    "rsf_preds[\"z_score\"] = (rsf_preds[\"risk_score\"] - rsf_preds[\"risk_score\"].mean()) / rsf_preds[\"risk_score\"].std()\n",
    "cox_preds[\"z_score\"] = (cox_preds[\"risk_score\"] - cox_preds[\"risk_score\"].mean()) / cox_preds[\"risk_score\"].std()\n",
    "\n",
    "# Define weights based on performance\n",
    "w_rsf = 0.745 / (0.745 + 0.72)  # Weight proportional to C-index\n",
    "w_cox = 0.72 / (0.745 + 0.72)\n",
    "\n",
    "# Weighted combination using z-scores\n",
    "combined_risk_score = (w_rsf * rsf_preds[\"z_score\"] + w_cox * cox_preds[\"z_score\"])\n",
    "\n",
    "# Store final risk scores\n",
    "final_preds = pd.DataFrame({\"ID\": rsf_preds.index, \"risk_score\": combined_risk_score})\n",
    "final_preds.to_csv(f\"./output/combined_risk_scores_zscore_{now}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "### Trying a DeepLearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models import CoxPH\n",
    "import torch\n",
    "from torchtuples.practical import MLPVanilla\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_t = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "X_test_t = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define a simple MLP model\n",
    "net = MLPVanilla(in_features=X_train.shape[1], num_nodes=[64, 32], out_features=1, batch_norm=True, dropout=0.1, output_bias=False).to(device)\n",
    "# print(y_train.dtype)\n",
    "\n",
    "y_train[\"OS_STATUS\"] = y_train[\"OS_STATUS\"].astype(int)\n",
    "y_train[\"OS_YEARS\"] = y_train[\"OS_YEARS\"].astype(float)\n",
    "\n",
    "# Train DeepSurv\n",
    "model = CoxPH(net)\n",
    "model.fit(X_train_t, y_train, batch_size=128, epochs=200, verbose=True)\n",
    "\n",
    "# Predict risk scores\n",
    "deep_risk_scores = -model.predict(X_test_t).cpu().detach().numpy()\n",
    "\n",
    "# Evaluate\n",
    "deep_cindex = concordance_index_ipcw(y_train, y_test, deep_risk_scores, tau=7)[0]\n",
    "print(f\"DeepSurv IPCW C-index: {deep_cindex}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
